# -*- coding: utf-8 -*-
"""Sentiment-Analysis01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iUntbVYrVhfcmiJYss3ldW3ugJt5w1Bh
"""

import pandas as pd

df = pd.read_csv('/content/Reviews.csv.zip')

print(df.info())

print(df.head())

df = df[['Score', 'Summary', 'Text']]

def convert_to_sentiment(score):
  if score <= 2:
    return "Negative"
  elif score == 3:
    return "Neutral"
  else:
    return "Positive"

df["Sentiment"] = df["Score"].apply(convert_to_sentiment)

print(df.head())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(6,4))
sns.countplot(x=df["Sentiment"], palette="coolwarm")
plt.title("Sentiment Distribution")
plt.show()

sentiment_counts = df["Sentiment"].value_counts()
print(sentiment_counts)

import seaborn as sns
plt.figure(figsize=(6, 4))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette="viridis")

plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.title("Distribution of Sentiments in Reviews")
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import nltk
import string
from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)

def preprocess_text(text):
  words = text.lower().split()
  words = [word for word in words if word not in stop_words and word not in punctuation]
  return " ".join(words)

df["clean_text"] = df["Text"].apply(preprocess_text)

sentiments = ["Positive", "Negative", "Neutral"]
colors = ["Blues", "Reds", "Greens"]
plt.figure(figsize=(15,5))

for i, sentiment in enumerate(sentiments):
    text_data = " ".join(df[df["Sentiment"] == sentiment]["clean_text"])
    wordcloud = WordCloud(width=800, height=400, background_color="white", colormap=colors[i]).generate(text_data)

    plt.subplot(1, 3, i + 1)
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"{sentiment} Reviews", fontsize=15)

plt.tight_layout()
plt.show()

import nltk
nltk.download('punkt')
nltk.download('stopwords')

import nltk

# Download required datasets
nltk.download('punkt')          # For tokenization
nltk.download('stopwords')      # For removing stopwords
nltk.download('wordnet')        # For lemmatization (if used)
nltk.download('omw-1.4')        # Extra support for wordnet

import pandas as pd
import string
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Ensure stopwords are downloaded
import nltk
nltk.download('stopwords')
nltk.download('punkt')

# Function to clean text properly
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    words = [word for word in words if word.isalnum()]  # Remove punctuation
    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords
    words = [word for word in words if len(word) > 1]  # Remove single letters
    return words

from nltk.tokenize import RegexpTokenizer

tokenizer = RegexpTokenizer(r'\b\w+\b')  # Extracts words only
words = tokenizer.tokenize(text)